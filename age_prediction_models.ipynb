{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ce8iQRutxUQs"},"outputs":[],"source":["# Installing requirements\n","! pip3 install transformers\n","! pwd"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/mfaisal/Documents/22_BackUp_BU/Colab Notebooks/ai-cs640/project/src\n"]}],"source":["! pwd"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":144,"status":"ok","timestamp":1638769294115,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"SN6vCCz9BIHT"},"outputs":[],"source":["# Import and imporatan initializations\n","import numpy as np\n","import pandas as pd\n","from pandas.io.json import json_normalize\n","# import torch\n","# import transformers\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.decomposition import PCA\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import preprocessing\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","\n","\n","import sys\n","import json\n","import os\n","import re\n","\n","def_random_seed = 42\n","np.random.seed(def_random_seed) # for reproducibility\n","\n","\n","# profiles_file_path = '/home/demographicPrediction/User_demo_profiles.json'"]},{"cell_type":"markdown","metadata":{},"source":["# Part one: declare assisting functions to load data."]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":161,"status":"ok","timestamp":1638765943675,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"2tnmxHHlr-6r"},"outputs":[],"source":["# remove links http or https\n","# remove usernames @User_Name92378\n","def removeLinksUserNames(x):\n","  regexMap={\n","      r\"http\\w*:\\/\\/[\\w*\\.*\\/*]*\\s\": \"\", \n","      r\"[\\w*:\\/]+\\.[\\.*\\w*\\/*\\~*]+\\s+\": \"\",\n","      r\"@[\\w_]+\\s\": \"\"\n","      }\n","  for regx in regexMap.keys():\n","    x = re.sub(regx, regexMap[regx], x)\n","  return x.lower()"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":213,"status":"ok","timestamp":1638768998927,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"ssNMQp7ZyCtr"},"outputs":[],"source":["# This file is a clean dataset that has the following:\n","# 290 user of age <= 21\n","# 290 user of age > 21\n","# Only english language\n","# ID - age - binary_age\n","def getAgeData(train_size = 500, max_tweets_per_example = 20, tokenizer = None, get_all = False):\n","  # age_labels_file_path = '/content/data/age_reduced_labeled_users.csv'\n","  # age_tweets_file_path = '/content/data/age_tweets.json'\n","\n","  age_labels_file_path = 'data/age_reduced_labeled_users.csv'\n","  # age_labels_file_path = 'data/age_labeled_users.csv'\n","  age_tweets_file_path = 'data/tweets.json'\n","\n","\n","\n","  np.random.seed(def_random_seed)\n","\n","  # Labels\n","  age_Y = np.genfromtxt(age_labels_file_path, delimiter=',', dtype=object)\n","  np.random.shuffle(age_Y)\n","  age_Y_train = age_Y[0:train_size, :]\n","  age_Y_test = age_Y[train_size:, :]\n","\n","  # Tweets\n","  with open(age_tweets_file_path) as tweets_file:\n","    tweets = json.load(tweets_file)\n","\n","  age_X_train = []\n","  age_Y_train_ = []\n","  age_X_test = []\n","  age_Y_test_ = []\n","\n","  for user in age_Y_train:\n","    user_name = user[0].decode(\"utf-8\")\n","    i = 0\n","    while (get_all and i < len(tweets[user_name])) or (i < int(len(tweets[user_name]) - max_tweets_per_example * 0.75)):\n","      age_X_train.append([removeLinksUserNames(tweet) for tweet in tweets[user_name][i:i+max_tweets_per_example]])\n","      age_Y_train_.append(user[2])\n","      i += max_tweets_per_example\n","  \n","  for user in age_Y_test:\n","    user_name = user[0].decode(\"utf-8\")\n","    i = 0\n","    while (get_all and i < len(tweets[user_name])) or (i < int(len(tweets[user_name]) - max_tweets_per_example * 0.75)):\n","      age_X_test.append([removeLinksUserNames(tweet) for tweet in tweets[user_name][i:i+max_tweets_per_example]])\n","      age_Y_test_.append(user[2])\n","      i += max_tweets_per_example\n","\n","  if tokenizer != None:\n","    age_X_train = [[tokenizer.encode_plus(tweet, return_tensors=\"pt\") for tweet in user_tweets ] for user_tweets in age_X_train]\n","    age_X_test = [[tokenizer.encode_plus(tweet, return_tensors=\"pt\") for tweet in user_tweets ] for user_tweets in age_X_test]\n","  \n","  age_Y_train_ = np.array(age_Y_train_).astype(np.float64).astype(np.int16)\n","  age_Y_test_ = np.array(age_Y_test_).astype(np.float64).astype(np.int16)\n","  \n","  return age_X_train, age_Y_train_, age_X_test, age_Y_test_"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":126,"status":"ok","timestamp":1638765945225,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"j1MRsnLtAYB3"},"outputs":[],"source":["# Get tweets embeddings\n","def getTweetsEmbeddings(X, max_tweets = 10, model = None):\n","  if model == None:\n","    model = transformers.AutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n","  \n","  layers = [-4,-3,-2,-1]\n","\n","  embedded_X = []\n","\n","  for i in range(len(X)):\n","    user_tweets = X[i]\n","\n","    tweet_embedding = []\n","    for j in range(len(user_tweets)):\n","      if j == max_tweets:\n","        break;\n","      tweet = user_tweets[j]\n","\n","      # Encode tweet\n","      with torch.no_grad():\n","        output = model(**tweet)\n","      states = output.hidden_states\n","      embedding = torch.stack([states[i] for i in layers]).sum(0).squeeze().numpy()\n","      tweet_embedding.append(embedding)\n","    \n","    embedded_X.append([embedding for tweet_embedding in tweet_embedding for embedding in tweet_embedding])\n","      \n","  return embedded_X"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":189,"status":"ok","timestamp":1638765946005,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"u2K2k1ziG9ZW"},"outputs":[],"source":["# Reduce tweet embeddings\n","def projectEmbeddingPCA(x, x_m, x_u):\n","  x_ = np.matmul(x - x_m, x_u)\n","  return x_\n","\n","def projectEmbeddings(X, word_size = 64, x_m = None, x_u = None, first_time = True):\n","  if first_time == True:\n","    # flatten embeddings\n","    X_flattened = np.array([tweet for tweets in X for tweet in tweets])\n","    \n","    # Get mean\n","    x_m = sum(X_flattened) / len(X_flattened)\n","\n","    # Normalize and covariance\n","    X_n = X_flattened - x_m\n","    X_c = np.matmul(np.transpose(X_n), X_n) / len(X_n)\n","\n","    # Get word_matrix\n","    x_u, x_s, x_Vh = np.linalg.svd(X_c)\n","    x_u_ = x_u[:, :word_size]\n","  else:\n","    x_u_ = x_u\n","\n","  X_ = [projectEmbeddingPCA(x, x_m, x_u_) for x in X]\n","  return X_, x_m, x_u_\n","\n","def reduceEmbeddingPCA(X, words_num = 5):\n","  X_t = np.array(X).T\n","  \n","  # Get the embedding mean\n","  X_m =  sum(X_t) / len(X_t)\n","\n","  # Normalize and covariance\n","  X_n = X_t - X_m\n","  X_c = np.matmul(np.transpose(X_n), X_n) / len(X_n)\n","\n","  u, s, Vh = np.linalg.svd(X_c)\n","  u_ = u[:, :words_num]\n","\n","  X_ = np.zeros((words_num, X.shape[1]))\n","  X__ = np.matmul(X_t, u_).T\n","  X_[0:X__.shape[1]] = X__\n","\n","  return X_\n","\n","\n","def reduceEmbeddings_(X, words_num = 5):\n","  X_ = np.empty((len(X), words_num, len(X[0][0])))\n","\n","  for i in range(len(X_)):\n","    X_[i] = reduceEmbeddingPCA(X[i], words_num)\n","\n","  return X_\n","\n","def reduceEmbeddings(X, word_size = 64, words_num = 5, x_m = None, x_u = None, first_time = True):\n","  X_, x_m, x_u = projectEmbeddings(X, word_size, x_m, x_u)\n","  X_ = reduceEmbeddings_(X_, words_num).reshape(len(X), words_num * word_size)\n","  return X_, x_m, x_u\n"]},{"cell_type":"markdown","metadata":{},"source":["# Part Two: explores the use of BERT-based encodings"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15091,"status":"ok","timestamp":1638765980866,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"JfzHHvJM7jb1","outputId":"af37ce28-3aec-4e17-d9c4-1c2a6582318a"},"outputs":[{"name":"stdout","output_type":"stream","text":["2890 (2890,) 457 (457,)\n"]}],"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","age_X_train, age_Y_train, age_X_test, age_Y_test = getAgeData(500, 15, tokenizer)\n","\n","# 48155 (500,) 7638 (80,)\n","print(len(age_X_train), age_Y_train.shape, len(age_X_test), age_Y_test.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1261205,"status":"ok","timestamp":1638767299346,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"vNzCoVUdFKFN","outputId":"d731f7f5-8fab-4c35-f4ff-d55a77a20f65"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["age_X_train_embeddings_300_15 = getTweetsEmbeddings(age_X_train[0:300], 15)\n","age_X_test_embeddings_300_15 = getTweetsEmbeddings(age_X_test[0:300], 15)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":40797,"status":"ok","timestamp":1638768342924,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"UgxdbXXJId7R"},"outputs":[],"source":["word_size = 16\n","words_num = 15\n","\n","age_X_train_embeddings_reduced, x_m, x_u = reduceEmbeddings(age_X_train_embeddings_300_15, word_size, words_num)\n","age_X_test_embeddings_reduced, _, _ = reduceEmbeddings(age_X_test_embeddings_300_15, word_size, words_num, x_m, x_u, False)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1189,"status":"ok","timestamp":1638768344095,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"I2NH-ve2QGHc","outputId":"b5ff4980-416c-4c4d-bc2e-81708de218a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.5959596  0.64912281 0.53333333 0.54685315 0.64517037]\n","0.5940878515412005\n"]}],"source":["# Linear Logistic Regression\n","# Train some model\n","model = LogisticRegression(max_iter=10000, warm_start = True)\n","scores = cross_val_score(model, age_X_train_embeddings_reduced, age_Y_train[0:300], cv=5, scoring='f1_macro')\n","print(scores)\n","print(np.mean(scores))"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":690,"status":"ok","timestamp":1638768344783,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"bbYQqjlKXLx2","outputId":"f019d3a0-807d-4af6-be11-ee979e8e75c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.45      0.39      0.42       160\n","           1       0.39      0.45      0.42       140\n","\n","    accuracy                           0.42       300\n","   macro avg       0.42      0.42      0.42       300\n","weighted avg       0.42      0.42      0.42       300\n","\n","[[63 97]\n"," [77 63]]\n"]}],"source":["# Test the model\n","model.fit(age_X_train_embeddings_reduced, age_Y_train[0:300])\n","y_pred = model.predict(age_X_test_embeddings_reduced)\n","y_props = model.predict_proba(age_X_test_embeddings_reduced)\n","\n","\n","print(classification_report(y_true=age_Y_test[0:300], y_pred=y_pred))\n","print(confusion_matrix(y_true=age_Y_test[0:300], y_pred=y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oj7j7LQKqCuY"},"outputs":[],"source":["# Probalilities show overfitting\n","print(y_props)"]},{"cell_type":"markdown","metadata":{},"source":["# Part Three: try models based on CountVectorizer and TF/IDF encodings:"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1443,"status":"ok","timestamp":1638770137800,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"6X0llvMKdIZY","outputId":"e4ae1bf5-958b-417f-d968-5438c42911d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["500 (500,) 80 (80,)\n"]}],"source":["# Step one: Preprocess the data\n","# Count Vectorized and Naive Bayes\n","age_X_train, age_Y_train, age_X_test, age_Y_test = getAgeData(500, 200, None, True)\n","print(len(age_X_train), age_Y_train.shape, len(age_X_test), age_Y_test.shape)\n","\n","# Flaten age_X_train\n","X_train_flat = []\n","for tweets in age_X_train:\n","  tweet_augmented = \"\"\n","  for tweet in tweets:\n","    tweet_augmented += tweet\n","  X_train_flat.append(tweet_augmented)\n","\n","X_test_flat = []\n","for tweets in age_X_test:\n","  tweet_augmented = \"\"\n","  for tweet in tweets:\n","    tweet_augmented += tweet\n","  X_test_flat.append(tweet_augmented)\n","\n","# vectorizer = CountVectorizer(stop_words='english')\n","# vectorizer.fit(X_train_flat)\n","\n","# age_X_train_ = vectorizer.transform(X_train_flat)\n","# age_X_test_ = vectorizer.transform(X_test_flat)"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175024,"status":"ok","timestamp":1638770667231,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"2jbwQZjHdIHR","outputId":"1efca842-b382-4659-a18c-00902d7c6519"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[0.67532468 0.6068152  0.69987995 0.61444805 0.69951923]\n","0.6591974226336458\n"]},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n","  warnings.warn(\n"]}],"source":["# Step two: trying the different following architectures.\n","# Train Model\n","# ['identity', 'logistic', 'relu', 'softmax', 'tanh']\n","# (16,4,1) >> 68%\n","model_nb = Pipeline([\n","                ('vect', CountVectorizer(stop_words='english')),\n","                ('tfidf', TfidfTransformer()),\n","                # ('standardscaler', preprocessing.StandardScaler(with_mean=False)),\n","                # ('pca', TruncatedSVD(4096)),\n","                # ('mlpc', MLPClassifier(hidden_layer_sizes=(16,4,1), max_iter=10000,activation = 'logistic', solver='adam',random_state=123)),\n","                # ('clf1', MultinomialNB()),\n","                # ('clf2', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n","                # ('clf3', LogisticRegression(n_jobs=1, C=1e5)),\n","                # (\"classifiers\", RandomForestClassifier()),\n","                (\"votingClassifier\", VotingClassifier(\n","                    estimators=[\n","                        ('mlpc', MLPClassifier(hidden_layer_sizes=(16,4,1), max_iter=10000, activation = 'logistic', solver='adam',random_state=123)),\n","                        ('clf1', MultinomialNB()),\n","                        # ('clf2', SGDClassifier(penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n","                        ('clf3', LogisticRegression(n_jobs=1, C=1e5)),\n","                        (\"classifiers\", RandomForestClassifier()),\n","                        ],\n","                    voting='soft',\n","                    weights=[\n","                        1,\n","                        1,\n","                        # 1,\n","                        1,\n","                        1,\n","                    ],\n","                    flatten_transform=True)\n","                ),\n","              ])\n","scores = cross_val_score(model_nb, X_train_flat, age_Y_train, cv=5, scoring='f1_macro')\n","print(scores)\n","print(np.mean(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60563,"status":"ok","timestamp":1638770734597,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"Lmq3DKzWfGeb","outputId":"a7ad064a-3ff5-445e-f899-71496be5e831"},"outputs":[],"source":["# Step three: test the chosen model\n","model_nb.fit(X_train_flat, age_Y_train)\n","y_pred_class = model_nb.predict(X_test_flat)\n","\n","print(\"Classification Report\")\n","print(classification_report(y_true=age_Y_test,y_pred=y_pred_class))\n","print(\"Confusion matrix:\")\n","print(confusion_matrix(y_true=age_Y_test,y_pred=y_pred_class))"]},{"cell_type":"markdown","metadata":{},"source":["# Part four : early attempts code and now not used anymore."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1638022654202,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"_gWiAqzZvL6i","outputId":"3c95989e-117d-48b0-a02e-3b24e1656c89"},"outputs":[{"name":"stdout","output_type":"stream","text":["      user_id  is_female  year_born  race\n","0     12488.0        0.0     1980.0   4.0\n","1    719703.0        0.0     1985.0   4.0\n","2    722153.0        1.0     1973.0   3.0\n","4    755531.0        0.0     1982.0   4.0\n","5    811618.0        0.0     1987.0   3.0\n","6    822540.0        0.0     1979.0   4.0\n","7    865071.0        0.0     1995.0   4.0\n","8    988211.0        0.0     1965.0   4.0\n","9   1025311.0        1.0     1955.0   4.0\n","10  1143891.0        0.0     1976.0   3.0\n"]}],"source":["# First step: read user labels and filter users\n","# Users data\n","# Header:   user_id  is_female  year_born  race\n","\n","# Read Labels\n","labels_raw = pd.read_csv(labels_file_path)\n","\n","# # Filters\n","id_filter = (labels_raw.user_id >= 0) & (labels_raw.user_id <= 9999999999)\n","gender_filter = (labels_raw.is_female == 0) | (labels_raw.is_female == 1)\n","year_filter = (labels_raw.year_born > 1900) & (labels_raw.year_born < 2020)\n","race_filter = (labels_raw.race >= 0) & (labels_raw.race < 5)\n","all_filter = id_filter & gender_filter & year_filter & race_filter\n","\n","# Filtered data \n","labels_filtered = labels_raw[all_filter]\n","\n","\n","# my_data = np.transpose(my_data)\n","# gender_dist = np.unique(my_data[1], return_counts= True)\n","# year_dist = np.unique(my_data[2], return_counts= True)\n","# race_dist = np.unique(my_data[3], return_counts= True)\n","\n","\n","# print(my_data.shape)\n","# print(labels_raw)\n","print(labels_filtered[0:10])\n","# print(gender_dist)\n","# print(year_dist)\n","# print(race_dist)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1072,"status":"ok","timestamp":1638022659566,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"AmYqoM94vL6j","outputId":"9af05cec-a6f5-46a5-c7f8-4e8ec9947f34"},"outputs":[{"name":"stdout","output_type":"stream","text":["3276\n","313473\n","[['12488', 'YKAR, a futuristic sans serif font by @EmmeranR - #Freebie #Font #Merci https://t.co/b6fBDvz6yZ'], ['12488', '@MBonvoyAssist Who can I contact about the very rude and poor service I’m experiencing during my current stay? Please and thank you!']]\n"]}],"source":["# Step 2: read tweets and arrange it in pair list\n","with open(tweets_file_path) as json_data:\n","    data = json.load(json_data)\n","\n","raw_tweets = []\n","for k in data:\n","    for v in data[k]:\n","        raw_tweets.append([k,v])\n","\n","print(len(data))\n","print(len(raw_tweets))\n","print(raw_tweets[0:2])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163211,"status":"ok","timestamp":1638022827336,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"b0GPHrQZvL6j","outputId":"f8548a6c-32d9-4d3e-eb7b-6987918f7edf"},"outputs":[{"name":"stdout","output_type":"stream","text":["(292114,)\n","['YKAR, a futuristic sans serif font by @EmmeranR - #Freebie #Font #Merci https://t.co/b6fBDvz6yZ']\n","[[   0 1980    4]]\n"]}],"source":["# Step 3: choose the tweets that has choosen users and create (tweet - is_female - year_born - race) array\n","tweets_X = []\n","tweets_Y = []\n","\n","for tweet in raw_tweets:\n","    user_info = labels_filtered[labels_filtered.user_id == float(tweet[0])].to_numpy()\n","    if len(user_info) > 0:\n","        tweets_X.append(tweet[1])\n","        tweets_Y.append(user_info[0,1:4].astype(int))\n","\n","tweets_X = np.array(tweets_X)\n","tweets_Y = np.array(tweets_Y)\n","\n","print(tweets_X.shape)\n","print(tweets_X[0:1])\n","print(tweets_Y[0:1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rn4CfO6PB8d"},"outputs":[],"source":["# Step 4: choose random train, test, validation sets\n","\n","# Train\n","train_indices = np.random.choice(tweets_X.shape[0], 140000, replace=False)\n","tweets_X_train = tweets_X[train_indices]\n","tweets_Y_train = tweets_Y[train_indices]\n","\n","# Validation\n","validation_indices = [x for x in np.random.choice(tweets_X.shape[0], 40000, replace=False) if x not in train_indices]\n","tweets_X_val = tweets_X[validation_indices]\n","tweets_Y_val = tweets_Y[validation_indices]\n","\n","# Test\n","test_indices = [x for x in np.random.choice(tweets_X.shape[0], 80000, replace=False) if x not in train_indices and x not in validation_indices]\n","tweets_X_test = tweets_X[test_indices]\n","tweets_Y_test = tweets_Y[test_indices]\n","\n","# save data \n","np.savetxt('/home/demographicPrediction/tweets_X_train.csv', tweets_X_train, delimiter=\",\", fmt='%s')\n","np.savetxt('/home/demographicPrediction/tweets_Y_train.csv', tweets_Y_train, delimiter=\",\")\n","\n","np.savetxt('/home/demographicPrediction/tweets_X_val.csv', tweets_X_val, delimiter=\",\", fmt='%s')\n","np.savetxt('/home/demographicPrediction/tweets_Y_val.csv', tweets_Y_val, delimiter=\",\")\n","\n","np.savetxt('/home/demographicPrediction/tweets_X_test.csv', tweets_X_test, delimiter=\",\", fmt='%s')\n","np.savetxt('/home/demographicPrediction/tweets_Y_test.csv', tweets_Y_test, delimiter=\",\")\n","\n","print(tweets_Y_train.shape[0])\n","print(tweets_Y_val.shape[0])\n","print(tweets_Y_test.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3725,"status":"ok","timestamp":1638107021307,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"lGqOmsyS1kvb","outputId":"7d98d6ec-e260-41f1-c23b-2a2972d64b9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["100\n","100\n","100\n"]}],"source":["# Step 4 (alt to 1-4): load chosen data\n","tweets_X_train = genfromtxt('/home/demographicPrediction/tweets_X_train.csv', delimiter='NOOOO DEL AT ALLL', dtype= np.str_)[0:100]\n","tweets_Y_train = genfromtxt('/home/demographicPrediction/tweets_Y_train.csv', delimiter=',', dtype= np.int16)[0:100]\n","\n","tweets_X_val = genfromtxt('/home/demographicPrediction/tweets_X_val.csv', delimiter='NOOOO DEL AT ALLL', dtype= np.str_)[0:100]\n","tweets_Y_val = genfromtxt('/home/demographicPrediction/tweets_Y_val.csv', delimiter=',', dtype= np.int16)[0:100]\n","\n","tweets_X_test = genfromtxt('/home/demographicPrediction/tweets_X_test.csv', delimiter='NOOOO DEL AT ALLL', dtype= np.str_)[0:100]\n","tweets_Y_test = genfromtxt('/home/demographicPrediction/tweets_Y_test.csv', delimiter=',', dtype= np.int16)[0:100]\n","\n","print(tweets_Y_train.shape[0])\n","print(tweets_Y_val.shape[0])\n","print(tweets_Y_test.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQQ4uLUm7ZNs"},"outputs":[],"source":["# Uses BERT to get words embeddings; then using PCA\n","# Reduces words embeddings to size of \"word_embedding_size\"\n","# Reduces tweet embeddings to size of \"tweet_embedding_size\"\n","# Returns projected X, PCA matrix for words, PCA matrix tweets\n","# Note: doing PCA on the current dataset is similar to\n","# fine tunning the BERT model\n","# TODO: get n words per tweet\n","def getTweetsBertEmbeddings(X, word_embedding_size = 64, tweet_embedding_size = 4, word_matrix = None, word_embedding_mean = None):\n","  # Use last four layers by default\n","  layers = [-4, -3, -2, -1]\n","\n","  # Getting tokenizer and pretrained model\n","  tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","  model = transformers.AutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n","\n","  # Getting encodings for a tweets\n","  encoded_tweets = [tokenizer.encode_plus(tweet, return_tensors=\"pt\") for tweet in X]\n","\n","  # Run model on encoded tweets\n","  tweets_embeddings = []                  # 3D matrix (tweet_num, tweet_vectors, word_vector)\n","  word_embeddings_num = 0\n","  for encoded_tweet in encoded_tweets:\n","    with torch.no_grad():\n","        output = model(**encoded_tweet)\n","    states = output.hidden_states\n","    embedding = torch.stack([states[i] for i in layers]).sum(0).squeeze().numpy()\n","    tweets_embeddings.append(embedding)\n","    word_embeddings_num+= len(embedding)\n","\n","  print(np.array(tweets_embeddings[0]).shape)\n","  print(np.array(tweets_embeddings).shape)\n","  print(np.array(np.array(tweets_embeddings).ravel()).shape)\n","  # First time call using training data\n","  if word_matrix == None or word_embedding_mean == None:\n","    # Flat all words embeddings through all tweets\n","    t_N = len(tweets_embeddings)\n","    t_words_N = word_embeddings_num\n","    t_word_size = len(tweets_embeddings[0][0])\n","    flattened_embeddings = np.array(tweets_embeddings).reshape(t_words_N, t_word_size)\n","\n","    # do PCA and choose best word_embedding_size\n","    # First normalize data\n","    word_embedding_mean = sum(flattened_embeddings) / flattened_embeddings.shape[0]\n","    flattened_embeddings_normalized = flattened_embeddings - word_embedding_mean\n","    flattened_embeddings_c = np.matmul(np.transpose(flattened_embeddings_normalized), flattened_embeddings_normalized) / flattened_embeddings_normalized.shape[0]\n","\n","    # Get word vector\n","    u, s, Vh = np.linalg.svd(flattened_embeddings_c)\n","    word_matrix = u[:, :word_embedding_size]\n","\n","    # For each tweet and for each word inside the tweet,\n","    # do the transformation into the new word space\n","    tweets_new_embeddings = np.array([np.matmul(tweet, word_matrix) for tweet in tweets_embeddings])\n","\n","    # For each tweet, each best tweet_embedding_size word vectors\n","    # using pca and then get to represent the new tweet using it.\n","    reduced_tweets = np.zeros((len(tweets_new_embeddings), tweet_embedding_size, word_embedding_size))\n","    for ind in range(len(tweets_new_embeddings)):\n","      tweet = tweets_new_embeddings[ind]\n","      tweet_c = np.matmul(tweet, np.transpose(tweet)) / tweet.shape[1]\n","      u_, s_, Vh_ = np.linalg.svd(tweet_c)\n","      tweet_matrix = u_[:, :tweet_embedding_size]           # old_words_num * new_words_num\n","      new_tweet = np.matmul(tweet.T, tweet_matrix).T\n","      reduced_tweets[ind] = new_tweet\n","\n","    return reduced_tweets, word_matrix, word_embedding_mean\n","\n","  else:\n","    tweets_new_embeddings = np.array([np.matmul(tweet - word_embedding_mean, word_matrix) for tweet in tweets_embeddings])\n","    \n","    # copied from above\n","    # For each tweet, each best tweet_embedding_size word vectors\n","    # using pca and then get to represent the new tweet using it.\n","    reduced_tweets = np.zeros((len(tweets_new_embeddings), tweet_embedding_size, word_embedding_size))\n","    for ind in range(len(tweets_new_embeddings)):\n","      tweet = tweets_new_embeddings[ind]\n","      tweet_c = np.matmul(tweet, np.transpose(tweet)) / tweet.shape[1]\n","      u_, s_, Vh_ = np.linalg.svd(tweet_c)\n","      tweet_matrix = u_[:, :tweet_embedding_size]           # old_words_num * new_words_num\n","      new_tweet = np.matmul(tweet.T, tweet_matrix).T\n","      reduced_tweets[ind] = new_tweet\n","\n","    return reduced_tweets\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24836,"status":"error","timestamp":1638107911994,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"aqxGeSJkuubz","outputId":"808f5ab9-1514-49ee-847c-0a61ba4df17d"},"outputs":[],"source":["# Step 5: Get Reduced tweets \n","word_size = 64\n","word_num = 5\n","reduced_tweets_train, word_u, word_m = getTweetsBertEmbeddings(tweets_X_train, word_size, word_num)\n","reduced_tweets_val = getTweetsBertEmbeddings(tweets_X_val, word_size, word_num, word_u, word_m)\n","reduced_tweets_test = getTweetsBertEmbeddings(tweets_X_test, word_size, word_num, word_u, word_m)\n","\n","print(reduced_tweets_train.shape)\n","print(reduced_tweets_val.shape)\n","print(reduced_tweets_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRAfZSSZxi0O"},"outputs":[],"source":["# Step 6: Add metrics functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTJ35kiuvP2o"},"outputs":[],"source":["# Step 7: Build the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4hj75_4vPtd"},"outputs":[],"source":["# Step 8: Evaluate the model\n","#   - Train the model\n","#   - Track "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64879,"status":"ok","timestamp":1637940276136,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"luTwpo1QvL6k","outputId":"abc90004-ac57-4a67-9ed9-5d8dbe91dac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[['YKAR', ',', 'a', 'futuristic', 'sans', 'serif', 'font', 'by', '@', 'EmmeranR', '-', '#', 'Freebie', '#', 'Font', '#', 'Merci', 'https', ':', '//t.co/b6fBDvz6yZ'], ['@', 'MBonvoyAssist', 'Who', 'can', 'I', 'contact', 'about', 'the', 'very', 'rude', 'and', 'poor', 'service', 'I', '’', 'm', 'experiencing', 'during', 'my', 'current', 'stay', '?', 'Please', 'and', 'thank', 'you', '!'], ['@', 'SSlnes', 'I', '’', 'd', 'like', 'to', 'win', '!'], ['@', 'LuckyDucksNFT', '@', 'Rydog'], ['Now', 'I', \"'m\", 'heading', 'to', 'B1000th', 'Floor', '!', '#', 'quickrogue']]\n"]}],"source":["# Now tokenize each tweet\n","import nltk\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","nltk.download('punkt')\n","\n","tweets_X_tokenized = [nltk.tokenize.word_tokenize(t) for t in tweets_X]\n","\n","print(tweets_X_tokenized[0:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sgADjI6vL6k"},"outputs":[],"source":["import numpy as np\n","import torch\n","import transformers\n","import os\n","\n","def get_word_idx(sent: str, word: str):\n","    return sent.split(\" \").index(word)\n","\n","def get_hidden_states(encoded, token_ids_word, model, layers):\n","    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n","    Select only those subword token outputs that belong to our word of interest\n","    and average them.\"\"\"\n","    with torch.no_grad():\n","        output = model(**encoded)\n","\n","    # Get all hidden states\n","    states = output.hidden_states\n","    # Stack and sum all requested layers\n","    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n","    # Only select the tokens that constitute the requested word\n","    word_tokens_output = output[token_ids_word]\n","\n","    return word_tokens_output.mean(dim=0)\n","\n","def get_word_vector(sent , idx, tokenizer, model, layers):\n","    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n","    that make up the word of interest, and then `get_hidden_states`.\"\"\"\n","    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n","    # get all token idxs that belong to the word of interest\n","    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n","\n","    return get_hidden_states(encoded, token_ids_word, model, layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2030,"status":"ok","timestamp":1637941545924,"user":{"displayName":"Muhammad Faisal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghk8phKlpm26Gwed8EMf-lyPnz6Vg08ggkcauLz=s64","userId":"12058286752193643018"},"user_tz":300},"id":"V7VRvg2lvL6l","outputId":"e819061b-cdbf-476f-dbc8-2356dc473a95"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[  101,   146,  1176, 18621,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n","768\n","tensor([[ 3.6612,  0.7498, -1.2082,  ..., -0.7623,  1.1960,  0.3004],\n","        [ 3.2393, -0.3290,  3.4289,  ...,  0.1825, -1.5243,  2.5883],\n","        [ 0.6567, -0.9944, -2.1942,  ...,  2.2502, -3.6377,  1.8441],\n","        [-0.5503,  0.1819,  4.4018,  ...,  1.7883,  0.3557,  3.4723],\n","        [ 2.3956, -1.1617,  0.7915,  ...,  1.0481,  0.5790, -0.1141],\n","        [ 0.7174,  0.3721, -0.0403,  ..., -0.0444,  0.9401, -0.4450]])\n"]}],"source":["\n","# Getting vectors for a sentence\n","sent = \"I like cookies .\" \n","encoded_sent = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n","print(tokenized_sent)\n","\n","# run model on encoded sentence\n","with torch.no_grad():\n","    output = model(**encoded_sent)\n","states = output.hidden_states\n","output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n","print(len(output[0]))\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFM0gps9vL6l"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"main.ipynb","provenance":[]},"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"},"kernelspec":{"display_name":"Python 3.9.0 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}
